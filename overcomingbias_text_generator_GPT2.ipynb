{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "overcomingbias-text-generator-GPT2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InquilineKea/AstroHackWeek2015/blob/master/overcomingbias_text_generator_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  overcomingbias.com text generator\n",
        "\n",
        "Notebook By [Alexis Clay](http://alexclay.io/). Forked from [GPT-2-Simple Colab](https://minimaxir.com/2019/09/howto-gpt2/) by [Max Woolf](http://minimaxir.com)  \n",
        "Blog posts (dataset) scraped by [ArielRoth](https://www.reddit.com/r/slatestarcodex/comments/g9ydnp/would_anyone_be_willing_to_train_gpt2_on_robin/fp8rmqx/?context=3)  \n",
        "\n",
        "You will be prompted to give autorization to your Google account for some actions, I offer no wearanty in terms of security/loss of data/etc. If you do not agree to do that, you can instead follow a [tutorial here](https://minimaxir.com/2019/09/howto-gpt2/) to learn to create your own Google Colab notebook safely.\n",
        "\n",
        "## To get started:\n",
        "\n",
        "## 1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "## 2. (recommended) Run the notebook in Google Chrome.\n",
        "## 3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "d8d9ccdd-10a4-4a69-a555-3a19fdfb4d2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## 4. Download GPT-2:\n",
        "\n",
        "You can choose either the `124M` or `355M` version:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M` (Used to train this colab): the \"medium\" model, 1.5GB on disk.\n",
        "* `774M` & `1558M`: Too large, cannot currently be used to finetune the blogposts with Colaboratory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "bf252eb4-9703-4ceb-ce4d-3142d9e995ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "model_name_loc=\"355M\"\n",
        "gpt2.download_gpt2(model_name=model_name_loc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 340Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 108Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 433Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:10, 137Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 218Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 127Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 170Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8sWwOuWnKK_",
        "colab_type": "text"
      },
      "source": [
        "## 5. Download trained checkpoint\n",
        "\n",
        "Two methods to do this:  \n",
        "**A-** Download this file ```https://drive.google.com/open?id=1XOJdpNcPH2PAQuHw7ZxEcBgwWh6BevN_```  \n",
        "then extract,  \n",
        "then click on ```files(icon) -> upload```  \n",
        "then upload the extracted tar into ```/content/checkpoint/robinhanson355m/[tar contents]``` in this Colab notebook.  \n",
        "\n",
        "**B-** run the following cells making sure to Enter credentials and give access when prompted, and it will upload the file to this Colab notebook automatically.\n",
        "\n",
        "(**Note:** Using either method, there's up to a minute delay until the tree is updated. Make sure to press the Files refresh button.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22bTT9IHM2OP",
        "colab_type": "text"
      },
      "source": [
        "- Authorize Google Drive (1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "828ff0e2-b65c-4424-d088-0b9d04e91230",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1zk0MlRbD-2",
        "colab_type": "text"
      },
      "source": [
        "- Authorize Google Drive (2):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMEnmkKUnOWk",
        "colab_type": "code",
        "outputId": "58c4dabb-1867-42b8-a24a-58daf1599a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.17.3)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.12.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.7.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.3)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sirMDvUmMz8p",
        "colab_type": "text"
      },
      "source": [
        "- Download:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17ANE-HunktZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "download = drive.CreateFile({'id': '1XOJdpNcPH2PAQuHw7ZxEcBgwWh6BevN_'})\n",
        "download.GetContentFile('checkpoint_robinhanson355m.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMI4S1yoMvYy",
        "colab_type": "text"
      },
      "source": [
        "- Refresh files (left tab), then create folder\n",
        "```checkpoint/robinhanson355m/``` and put the new tar file inside it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3NAzbk9bLxP",
        "colab_type": "text"
      },
      "source": [
        "- Extract tar file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrbq7bEePcMi",
        "colab_type": "code",
        "outputId": "318e6d03-5aab-4333-b5d8-f621f3510512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "source": [
        "!tar -xvf /content/checkpoint/robinhanson355m/checkpoint_robinhanson355m.tar"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "tar: checkpoint: Cannot open: File exists\n",
            "counter\n",
            "encoder.json\n",
            "events.out.tfevents.1588489121.4675894d9b67\n",
            "events.out.tfevents.1588489676.4675894d9b67\n",
            "events.out.tfevents.1588552434.34878838a096\n",
            "events.out.tfevents.1588554732.34878838a096\n",
            "hparams.json\n",
            "model-1497.data-00000-of-00001\n",
            "model-1497.index\n",
            "model-1497.meta\n",
            "vocab.bpe\n",
            "tar: Exiting with failure status due to previous errors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfexa7KLzvGI",
        "colab_type": "text"
      },
      "source": [
        "## 6. Run text generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuRc16l0SASh",
        "colab_type": "code",
        "outputId": "26fde61d-dc14-46cd-b85a-08368c3061f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# RUN THIS CELL ONLY ONCE. If you want to restart \"Runtime->Restart Runtime\"\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='robinhanson355m')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint checkpoint/robinhanson355m/model-1497\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/robinhanson355m/model-1497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "2e09b9c1-9d04-448b-a2fc-4bcf67287982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "         \n",
        "# this is where you give the model a starting text to generate results along\n",
        "# the same topic. you can also leave it as an empty string        \n",
        "## start of prompt\n",
        "prefix_gen = \"\"\"Title: Not All Lies\n",
        "Date: June 3, 2012 2:30 pm\n",
        "Tags: Disagreement, Web/Tech\n",
        "Number of comments: 23\n",
        "\n",
        "People often like to believe that they believe on behalf of\"\"\"\n",
        "## end of prompt\n",
        "\n",
        "gpt2.generate(sess,\n",
        "              prefix=prefix_gen,\n",
        "              run_name='robinhanson355m',\n",
        "              model_name=model_name_loc,\n",
        "              top_p=0.9,\n",
        "              length=1023,\n",
        "              temperature=0.7,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "“I think it would be very hard to convince voters to adopt a policy that they would find it hard to implement. … A policy can only be approved if it is accompanied by a plan to make the policy work. … The more difficult it is to sell the policy, the less support the policy will get. … The second piece of evidence is that the policy is unpopular.  … If the policy is unpopular, voters will not adopt it.  Third, we see that policies that increase voter participation tend to be unpopular as well. … It seems that policies that increase voter participation are also less effective at increasing voter turnout.  So in the end, increased voter participation just tends to decrease the vote share of the less popular policies.  This is a pretty basic observation.  It is one of the many reasons, for example, we need universal suffrage or high tariffs.  And it is one of the reasons, I think, why free speech is so important. We need people to be able to express themselves without fear of violence. … We are willing to subsidize speech that we don’t particularly like, but we don’t want our speech to be overwhelmingly liked.  And we don’t really want the government telling us what we can and can’t say, since that would be like telling the foxes what to say.\n",
            "I think this helps explains why I’m not into politics.  Politics isn’t about policy, after all.\n",
            "<|endoftext|>\n",
            "\n",
            "Title: Is Self Assured Destruction\n",
            "Date: January 20, 2009 6:00 am\n",
            "Tags: Disaster\n",
            "Number of comments: 13\n",
            "\n",
            "Yesterday I heard a talk by a neurobiologist who specializes in mental health crises.  He said that our immune system destroys our brain by destroying our white blood cells, but we don’t know why.  We do know that our immune system destroys our white blood cells in large quantities when we are sick.  But the mechanism that destroys our white blood cells in our bodies is a bit muddled.  Apparently when we are stressed, our bodies make a protein that paralyzes other cells, but that paralyzes itself when it senses danger.  This paralyzes white blood cells too, apparently causing them to become white.  This paralyzes the body, which then makes a white protein.  This white protein then paralyzes the heart, and so on.  Does anyone have a better handle on this problem than I do?\n",
            "This muddled theory is interesting, but I have to admit it seems unlikely to me that our immune system is mainly concerned with white blood cells.  The whole point of our immune system is to protect us from disease, not to fight white blood cells.  We have white blood cells because our bodies have been trying to protect us from disease.\n",
            "Yes our immune system has a secondary function, to kill bad cells, but that secondary function is pretty limited.  I could see a way for our immune system to fulfill that secondary function, but that seems rather unlikely.  Our brain has many white blood cells, but they are not primarily concerned with fighting disease.\n",
            "Yes our immune system is stimulated by the threat of disease, but that is not the main function of our immune system.  Our immune system can signal to other immune cells to raise or lower their intensity when it senses that other cells are about to be harmed or killed.  Our brain can also signal that it is about to be harmed or killed, but that is of course secondary to trying to signal other immune cells when it senses that other cells are about to be harmed or killed.\n",
            "Yes our immune system can sometimes signal our body that it is about to be harmed or killed, but that is mostly a side effect of the signal our body is trying to send to other immune cells.\n",
            "Yes our immune system can sometimes be activated by signals our body is sending to other immune cells, but those signals are secondary to the signal that our body is sending to other cells when it senses that other cells are about to be harmed or killed.\n",
            "Yes our immune system can sometimes activate just by signalling to other immune cells, but that is mainly a side effect of the signal our body is sending to other cells when it senses that other cells are about to be harmed or killed.\n",
            "Yes our immune system can sometimes activate by killing bad cells, but that is secondary to trying to signal to other cells when it senses that other cells are about to be harmed or killed.\n",
            "Yes our immune system can sometimes kill bad cells when it senses that others are about to die, but that is secondary to signalling to other cells when it senses that others are about to die.\n",
            "Yes our immune system can sometimes activate to kill bad cells, but that is secondary to trying to signal to other cells when it senses that others are about to die.\n",
            "Yes our immune system can sometimes kill bad cells by itself, but that is secondary to signalling to other cells when it senses that others are\n",
            "====================\n",
            "“So the next time you tell a group of friends you just met that you just bought a new car, or you tell a boss you just won a big promotion, remind them that you just bought a car, or won a promotion, or saved for a new one, or whatever, they might just believe you.”\n",
            "\n",
            "So, do you buy cars, or save for new ones?  I bet you don’t, because you seem so eager to believe that you just saved for a car.  But you don’t if you save for a new car because you just bought one, you just raced your friends’ dogs, or whatever.  Why do you think you save for cars, if not for new ones?\n",
            "Let’s imagine some other group of friends, or bosses, or other group members, who were more skeptical than you about the value of your saving.  Would you believe them, or treat them as less skeptical?\n",
            "If you think about it, the key thing that distinguishes you is your belief about the relative value of your saving vs. other saving.  For example, if you think your other saving is worth more than yours, then on net your other saving is more valuable than yours.  You might think you saved for a new car because you needed to get somewhere, but you don’t really care much about that; it wasn’t important to you.  You might think you saved for a new computer because you were cool, but you don’t really care about that either; it wasn’t important to you.\n",
            "Now consider some group members who are more skeptical than you about the relative value of your saving vs. such saving.  Would you believe them, or treat them as less skeptical?\n",
            "If you think about it, the key thing that distinguishes you is your belief about the relative value of your belief vs. the relative value of your saving.  For example, if you think your belief about the relative value of your belief is more valuable than others’, then on net your belief is more valuable than others’.  You might think you saved for a car because you were cool, but you don’t really care much about that; it wasn’t important to you.  You might think you saved for a car because it was cool, but you don’t really care about that either; it wasn’t important to you.\n",
            "If you think about it, the key thing that distinguishes you is your belief about the relative value of your belief vs. the belief that others have.  For example, if you think your belief about the relative value of your saving is more valuable than others’, then on net your belief is more valuable than others’.  You might think you saved for a car because you were cool, but you don’t really care about that; it wasn’t important to you.  You might think you saved for a car because it was cool, but you don’t really care about that either; it wasn’t important to you.\n",
            "If you think about it, the key thing that distinguishes you is your belief about the relative value of your belief vs. the belief that others have.  For example, if you think your belief about the relative value of your saving is more valuable than others’, then on net your belief is more valuable than others’.  You might think you saved for a car because you were cool, but you don’t really care about that either; it wasn’t important to you.  You might think you saved for a car because it was cool, but you don’t really care about that either; it wasn’t important to you.\n",
            "\n",
            "I’ve argued that the key to rationality is identifying the beliefs that others have, and then identifying the beliefs that you hold.  If you have a non-belief that you’d like to add to this group, you have to decide what is the least likely belief that it could add to the group, and if any) you’d like to have an especially low belief.  If you don’t have a belief yet, you might as well set out to find one.\n",
            "Of course it is possible that you will find a non-belief that you’d like to have.  You might start out by trying to find a belief that others have, but then decide that it is not worth the trouble to add to this group.  After all, if it were easy, you’d find beliefs that many have.  But this is not how it works in a world of relative beliefs.  You are first “found” by an ally who is willing to go further than you.  You\n",
            "====================\n",
            "Why Study History?\n",
            "\n",
            "Why study history when so many other subjects are telling you to study economics?  The answer, I think, lies in the same thing: we want to be convinced that we need to study something, so that we stay in school and gain the credentials we need to be hired and elected and all the other privileges we aspire to?  Yes, we want to be certified as being in the \"professions\" in which we practice.  But we also want to be believed by our peers, especially our teachers, that we are doing well in our chosen subject.  And we want to be admired by our significant other, especially if that subject involves us in our professional lives.  Why study history if you don’t want to be believed?  Why study economics if you don’t want to be admired?  I’m sure we can find similar excuses in most any other field.\n",
            "<|endoftext|>\n",
            "\n",
            "Title: Inequality Is Down\n",
            "Date: August 19, 2009 6:00 am\n",
            "Tags: Charity, Finance, Inequality\n",
            "Number of comments: 18\n",
            "\n",
            "We seem content to think up reasons why our decisions suck, and then try to convince ourselves that these reasons don’t apply to us.  Inequality is a big reason why.\n",
            "But if we thought up reasons why some decisions suck more than others, we would have to come up with reasons why it is certain subjects are more deserving of investment than others.  Sure some subjects are more deserving of charity than others, but why must we choose between these equally worthy causes?\n",
            "Surely some subjects are more deserving of investment than others, and if we are considering investments we are thinking in terms of some supposed \"utility function\" of the subject.  But if we are considering projects to build stuff, and projects are just tools we are using to achieve some task, then why must we think in terms of some supposed efficiency of the subject?\n",
            "Surely some subjects are more deserving of respect than others, and if we are considering subjects responsible for important social behaviours, then we must think in terms of some supposed respect for the subjectivity of these supposed \"decision makers\".  Sure some subjects are more deserving of money than others, but why must we give to all equally deserving subjects?\n",
            "Surely some subjects are more deserving of praise than others, and if we are considering subjects celebrated for their supposed creative contributions to science, technology, or other fields, then we must consider subjects as if they were merely amusing or educational.  But why must we give to all equally deserving subjects?\n",
            "Surely some subjects are more deserving of media attention than others, and if we are considering subjects with media attention we are thinking in terms of some supposed media utility function.  Sure some subjects are more deserving of academic attention than others, and if we are considering academic subjects to gain a better understanding of some supposed analytic problem, then we must frame some sort of relation between subject and solution that makes it clear which is the more important subject.\n",
            "Surely some subjects are more deserving of Nobel prizes than others, and if we are considering a subject whose prize is greater than some other, then we must think prize-centeredly, considering the most valuable prize as a reward for some discernible contribution.  But why must we give to all equally deserving subjects?\n",
            "Yes prizes can signal respect for the winner, but surely it is far from clear that they are the most important signal for signaling respect for others?  Why must we give to all equally worthy subjects?\n",
            "<|endoftext|>\n",
            "\n",
            "Title: Inequality Is Down\n",
            "Date: August 18, 2009 6:00 am\n",
            "Tags: Charity, Finance, Inequality\n",
            "Number of comments: 24\n",
            "\n",
            "Back in January I posted:\n",
            "\n",
            "The supply of pizza is almost exhausted, yet we still enjoy eating it.  Why?  I’d argue that people treat it like a social good: when others give, others take credit. When others don’t give, others blame. … When pizza is unavailable, we feel tempted to steal. (HT to my brother.)  … If it weren’t for social welfare nonprofits, we’d be eating pizza less often. …\n",
            "Now consider the inequality created by our love of pizza.  Most folks consider it rude to leave a meal tipsy, but we love pizza in equal measure – on our wedding night, for example.  Yet we see plenty of instances of rude tipping, such as at fast-food joints.  Yet we see far fewer instances of generous tipping – people simply don’t tip enough.\n",
            "Now consider the all the other activities folks like to participate in.  Music, dancing, picnics, group picnics – do these activities encourage people to put in extra effort?  Do they even reward effort?  Do they even reward volunteering?  Do they even reward constructive criticism?  If you think for a moment\n",
            "====================\n",
            "“Just one more reason that my job is to make sure the best people are treated fairly.”\n",
            "<|endoftext|>\n",
            "\n",
            "Title: What Is Public\n",
            "Date: October 20, 2009 6:00 am\n",
            "Tags: Current Affairs, Law, Politics\n",
            "Number of comments: 20\n",
            "\n",
            "The Post reminds us that Bush-era tax cuts for the rich were not about redistribution:\n",
            "The current crop of Republican presidential hopefuls have seized on a report by the Organization for Economic Cooperation and Development to argue that the current crop of Republican presidential hopefuls have seized on a report by the OECD to argue that economic growth is on track and that, if elected, they would double the child credit and lower the corporate rate.  … The O.E.D. study, which compared the growth rates of member countries from 1995 to 2005, concluded that the U.S. grew at an annual rate of 3.9 percent between 1995 and 2005. Canada, Mexico and Poland each saw growth rates of 2.5 percent. …\n",
            "Mr. Bush’s remarks … were quickly panned by analysts and challenged by some Republicans on Capitol Hill.  … “If you look at the OECD’s assessment of the economic performance of the past 50 years, (the top tax rate) stands at 70 percent, (the top marginal rate) stands at 39 percent, (the top rate on investment) stands at 3 percent, (the bottom rate) stands at 12 percent, (and) cumulative national income per capita exceeds that of most developing countries,” said Representative George Miller, Democrat of California and co-author of the study. …\n",
            "“These kinds of simplistic economic studies are used by (Major League Baseball) to try to inflate their player salaries and (NCReal Valley Yum!) to try to try to try to get players to come to the U.S. to play.” … Mr. Miller said the OECD considers its methodology and findings only to the extent that they are verified by other studies.\n",
            "So the OECD just repeated a discredited finding that was clearly refuted by many others – that U.S. growth rates were unusually high in recent decades.  The obvious question is: why the disdain for this finding?  We already have a public policy “fix” for major problems: tax at will, especially for the very rich, to stimulate growth.  Why not for the at risk but no less risky private policies that affect lots of other folks?  I have suggested:\n",
            "\n",
            "The public doesn’t really like to see big broken promises, especially broken ones about big important things.\n",
            "The public dislikes to see policy imposed by powerful elites, such as big league stars, takes a long time to acclimate to, and makes the system untenable.\n",
            "The public dislikes to see policy chosen by insiders, such as by insiders of the major leagues, requiring long term commitments that aren’t economically feasible.\n",
            "\n",
            "<|endoftext|>\n",
            "\n",
            "Title: Reform Isnt About Policy\n",
            "Date: October 17, 2009 9:00 pm\n",
            "Tags: Current Affairs, Politics\n",
            "Number of comments: 60\n",
            "\n",
            "We care more about stuff we can see and touch, and stuff we can’t see or touch, than we do about stuff we don’t have access to.  We care more about stuff we can’t touch and see, and can’t touch, than we do about stuff we do have access to.  Transparency, social disapproval, and accountability would seem to us to reduce the latter by enhancing the former.   Yet we actually celebrate the former!\n",
            "So why do we tolerate less transparency on policy outcomes, relative to stuff we can touch and see?   One explanation is that we care more about policy info leakage, via our policy fakers.   Less attention is paid to info leakage about which fakers are who, and how to avoid such leaks.   But this explanation doesn’t seem to explain our more open-minded attitude toward policy info; we are perfectly happy to sacrifice our fakers for the greater good of info leakage!\n",
            "So what do you think is behind this openness about policy fakers?\n",
            "<|endoftext|>\n",
            "\n",
            "Title: Future Incompetence\n",
            "Date: October 16, 2009 6:00 am\n",
            "Tags: Future, Regulation\n",
            "Number of comments: 34\n",
            "\n",
            "The [US Firms] Ease of Work Stress test assesses firms for their ability to set and maintain productive work environments.  A positive score indicates that firms are able to set and maintain productive work environments; negative scores indicate that they are not able to set and maintain productive work environments.  A positive result would indicate that firms set and maintain productive work environments more often, while a negative score would indicate they are less able to set and maintain productive work environments.  Apparently this second result is the result that the [US] Firms dislike, since\n",
            "====================\n",
            "Hanson said that \"the list of potential cures, if discovered, would be long and would include everything from eye surgery to artificial sweeteners.\" … He’d also love to see the federal government \"take a piece of the drug market with all the excess revenue that it represents and make it a healthcare reform program.\" … [He] would also love to see the Internal Revenue Service \"revenue-tax all sugar-sweetened beverages sold in the United States.\"  … [He] would also love to see \"a tax on the excess veterinary-medicine expenses paid by pet owners who don’t keep their pets on a \"normal\" diet.\"  … [He] supports taxing the \"excess food wasted by consumers in the United States\" and \"introducing a flat tax with a value on food.\"  (more)\n",
            "\n",
            "I’d like to see a flat tax with a value on food, but that seems pretty hard to justify.  Any other plausible reasons to expect less health care inequality?\n",
            "Added 11p: The study above was published in the June Journal of Health Economics.  It controls for many other factors that affect health.\n",
            "<|endoftext|>\n",
            "\n",
            "Title: How To Tax Lies\n",
            "Date: June 13, 2009 6:00 am\n",
            "Tags: Disagreement, Politics\n",
            "Number of comments: 23\n",
            "\n",
            "Imagine you are a board member of an organization that believes in climate change.  You have heard various estimates about how much it will cost to cut back on some unknown area of activity, like space exploration.  You consider which areas might actually be cut, and which might incur higher costs.  You also consider which could actually be a net gain, like improved health.  And at the end of the process you still don’t know which areas will actually be cut.\n",
            "Now consider that in a lot of industries you can usually find figures that estimate costs, benefits, and the marginal benefits to be worth a dollar of added cost.  You can trust such figures, because they are more interested in looking good than in making accurate costs and benefits.  You don’t have to believe such costs and benefits are obviously worth a dollar, but you do have to be willing to accept a million or a trillion dollars of lost activity for such figures to make accurate costs and benefits estimates.\n",
            "Similarly imagine that a similar firm, but this time on topic you are considering, is actually willing to actually make cost and benefit estimates.  The topic is not politically controversial, and is not especially controversial in the industry.  But you are willing to make actual cost and benefit estimates, even if the official cost and benefit estimates are of the opposite sides of a political spectrum.  You might expect that given a similar situation, such a firm would probably also prefer to actually estimate costs and benefits.  And if you gave such a firm a set of relevant abstractions, it would probably make similar assumptions.  But what if the topic was not politics, but instead a set of abstractions on how to tax lies?\n",
            "For example, if you thought it obvious that it is usually better to tax statements where prices are lower, you might well accept the following abstractions:\n",
            "\n",
            "Truth – prices are lower\n",
            "Fraud – statements are more likely to be false\n",
            "Deception – statements are more likely to be false\n",
            "Marketing – prices are higher\n",
            "\n",
            "While you might also accept:\n",
            "\n",
            "Fraud – misleading statements are more likely to be true\n",
            "Market Manipulation – statements are more likely to be set lower\n",
            "Market Making – statements are more likely to be true\n",
            "\n",
            "Which of these do you prefer?  That is, do you prefer the appearance of being able to choose your own truths, or the appearance of being controlled by others who want you to believe certain things?\n",
            "I’ve been trying to think about this lately, and I come up with the following intuitive answer:  the appearance of controlling others via appearance is just as bad as the other control mechanisms.  \n",
            "<|endoftext|>\n",
            "\n",
            "Title: Overconfidence Explained\n",
            "Date: June 12, 2009 6:00 am\n",
            "Tags: Overconfidence\n",
            "Number of comments: 8\n",
            "\n",
            "A meta-analysis of 18 studies of overconfidence, … In the meta-analysis, more highly credentialed researchers produced more conservative estimates. But when researchers were allowed to use more meaningless words, such as “self-regulation” or “cognitive load”, their estimates fell. The credentialed researcher’s relative overconfidence should therefore be higher in these cases. (more)\n",
            "This has just been confirmed in a 2005 paper, and was confirmed in a  Psychological Science article last year.   Here is how it works:\n",
            "People with higher levels of expertise are more likely to use words and arguments that are likely to be understood by others. This appears to overcome some of the superiority signaling that occurs when expertise is concentrated in a\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZpg1InlZqjn",
        "colab_type": "text"
      },
      "source": [
        "## (The above should satisfy your curiosity for generating text. The rest of this notebook is optional. It has cells for fixing rare issues or cells I used to train the model)\n",
        "--------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "3c0f4624-4c4b-4963-df53-b252abfe821a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=\"overcomingbias.txt\",\n",
        "              model_name=model_name_loc,\n",
        "              steps=4000,\n",
        "              batch_size=5,\n",
        "              restore_from='latest',\n",
        "              run_name='run2',\n",
        "              print_every=10,\n",
        "              save_every=100,\n",
        "              sample_every=200\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint checkpoint/run2/model-800\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run2/model-800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.06s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1161358 tokens\n",
            "Training...\n",
            "Saving checkpoint/run2/model-800\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "“Picking a fight with the left” means arguing with someone who has the left’s political views, instead of the other way around. … Most leftists do not even know that “Poker looks bad in photos.' And some, such as New York Magazine”, even complain “the photos tend to whitewash the left” on everything from paid sick leave to campus rape.\n",
            "As I wrote two months ago:\n",
            "Leftists like to say conservatives “only like to push their beliefs on others,” but in fact liberals and conservatives both exploit each other more. Liberals push their status and identity by arguing their points, as do conservatives by pushing theirs. I can only hope that in the future liberal activists will realize that it is far more effective to push their beliefs on libertarians than on other liberals.\n",
            "More quotes from the book:\n",
            "Leftists love to point out that the left is all about equality, and that all ideologies stem from the left; both Sartre and Foucault were socialists. But when leftists criticize the left, they usually mean the other side is less about equality and more about racial equality. Liberals like to protest that conservatives value freedom and individual initiative too much, but a lot of the trouble of the conservative revolution was precisely what you see liberals complaining about in the left. They believed in freedom and were against racism, but not white flight and the end of Jim Crow. Liberals hate to be outwitted, but the left never backs down when pitted against racists.\n",
            "A leftist is someone who might be mistaken for an idealistic egalitarian, but genuinely believes the nation is more about race than equality. … [They’re] someone who, like me, values freedom and initiative too highly. … Right-wingers want to tell you we’re all the same. … We’re all aristocrats now. … They can’t get elected. … All life is a struggle between the productive and the parasitic. … No one should dare to be in the habit of thinking that all these people are equal.\n",
            "“Heresy,” the [French] economist Thomas Coase once told a friend, “A right-winger is no better than a socialist.” Coase went on, however, to distinguish between the two kinds of people: The right-winger and the social democrat were both just doing what the socialists were doing, the wrong sort of thing, and neither was a better economist. …\n",
            "Coase’s point should remind us that, far from being just and reasonable, the claim to be the best economist of all times is a boast, and we conservatives should be ashamed of ourselves if we refuse to enforce it. We conservatives insist that economists, like most other folks, are mostly right all the time, though we are mistaken many times. … For most economists, the claim to be the best is a status symbol, not a claim to be honest and decent. …\n",
            "Coase’s [description of the American experiment] answer is simple. “If there were no blacks, the experiment would not have been repeated.” We should not repeat it. … A good answer to the Coase question is … do people think differently if there are no blacks or not? If yes, is that because people have internalized so many Coasean errors? Probably not. It may just be that people have absorbed too many Coasean errors to unlearn them. …\n",
            "Coase’s solution to this paradox, which is hardly a new one, is to recognize that “innovative” people tend to be badly corrupted by their cultural background, which makes them less informed. But such an answer will face a problem, because it is a recognition of the immorality of our cultural heritage … The new problem, then, is how do we make it more effective so that we don’t repeat the experiment. Is there an obvious starting place there? Yes, surely genetic engineering is one area where we might be more informed. It is also possible to do some things, such as the timing of tests for genetic disease, that will reduce the amount of corruption that is being introduced into the process.\n",
            "But what of the other side? For one thing, many cultural variations are caused by different problems from our culture, such as bad parents, bad education, poor parenting, and so on. … In some cases it might make sense to institute a policy of racial integration in order to preserve cultural diversity. But, if we don’t want to admit our mistakes, the best place for us to admit them is in the institution of theocracy. We could abolish theocracy tomorrow and get worse. What policy would be better?\n",
            "<|endoftext|>\n",
            "\n",
            "Title: Is This Black Or White\n",
            "Date: December 7, 2010 4:30 pm\n",
            "Tags: Politics, World\n",
            "Number of comments: 25\n",
            "\n",
            "We are more Democratic (i\n",
            "\n",
            "[810 | 135.32] loss=2.07 avg=2.07\n",
            "[820 | 213.20] loss=1.82 avg=1.94\n",
            "[830 | 292.55] loss=1.87 avg=1.92\n",
            "[840 | 373.72] loss=2.06 avg=1.95\n",
            "[850 | 455.03] loss=1.28 avg=1.82\n",
            "[860 | 536.93] loss=2.02 avg=1.85\n",
            "[870 | 619.21] loss=1.77 avg=1.84\n",
            "[880 | 701.81] loss=1.72 avg=1.82\n",
            "[890 | 784.11] loss=1.54 avg=1.79\n",
            "[900 | 866.40] loss=1.35 avg=1.74\n",
            "Saving checkpoint/run2/model-900\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "[910 | 954.32] loss=1.70 avg=1.74\n",
            "[920 | 1035.89] loss=1.64 avg=1.73\n",
            "[930 | 1117.81] loss=1.23 avg=1.69\n",
            "[940 | 1199.95] loss=1.60 avg=1.68\n",
            "[950 | 1282.22] loss=1.14 avg=1.64\n",
            "[960 | 1364.85] loss=1.53 avg=1.64\n",
            "[970 | 1447.33] loss=1.50 avg=1.63\n",
            "[980 | 1529.98] loss=1.24 avg=1.60\n",
            "[990 | 1612.07] loss=1.44 avg=1.59\n",
            "[1000 | 1694.51] loss=1.37 avg=1.58\n",
            "Saving checkpoint/run2/model-1000\n",
            "======== SAMPLE 1 ========\n",
            " which they are.  In particular, the \"moral\" aspect of their life and death matters more to them than most.  So they do what seems right in theory, but in practice makes the most of actual circumstances, over and above the ideals they might otherwise have set.  It’s a very human thing to do, even when it’s \"not in our vocabulary\" or \"not part of the dictionary\" or \"not on our shopping lists.\"\n",
            "\n",
            "I don’t dispute this general theoretical premise; the only disagreement I can see is if idealism was the important factor behind the altruism in these particular cases.  If so, a common cause-effect relationship among altruism, theory, and practice would be a relatively strong force and easily explainable.  However, I have not found such a pattern, nor do I see any in the literature on people my age group who say \"I already give about US$100 a month or more to help fight malaria.\"  And I doubt many would set aside money to help fight malaria either; many people consider money in the bank mainly as a way to save, and there are still lots of unmeasured moral advantages to getting help from others over having others help you.\n",
            "Added:  When one person gives malaria-immune relative very much money to a child who also gets money from another family member, the child gets no money at all from the parent.  Since the child also gets money from someone else, the situation is reversed.  It might seem self-serving to leave the child’s parents completely penniless, but it is morally right. \n",
            "<|endoftext|>\n",
            "\n",
            "Title: Newer Firms Wont Share\n",
            "Date: September 12, 2009 3:15 pm\n",
            "Tags: Innovation\n",
            "Number of comments: 18\n",
            "\n",
            "Imagine we ran a study where we asked firms whether they would let younger people work in their organization, and then allowed or required them to fire people.  Our question was whether some people would be happy to work at such a company, and if so how easy it would be for this generation to replicate their success.  If companies said \"no way\", it would mean older people could not be trusted to manage their companies like older people had their earlier generation.  So we had some firm execs tell us they wouldn’t let younger folks into their organizations.\n",
            "I’ve had several chance conversations with ex-coach/co-signers at VC firms who are now working in a variety of startup settings.  They tell me of course, many of them are horrified to see that their old rival will make such a huge change by spreading themselves thin, but most are not so quick to align themselves with and join such a new \"no bullshit\" environment.\n",
            "It is not clear to me why VC firms would reject a risk of having their best employee fire, just as no one can see that fire would be profitable.  Some kind of risk aversion must be part of it.   My guess is that VC firms see young firms as especially easy to replace and make much more money in the long run from being first.    Also, they see those with only \"a decade left to live\" as more deserving of their wrath, relative to being next year’s victim who is just being harassed.\n",
            "For the record, my father was a cop -- I grew up next door in a middle-age neighborhood where our houses were next to each other.    It seems obvious that many of our culture’s strongest memories of our parents include police abuses.  Yet today most VC firms are in rural or suburban office parks or other small towns, and are much less densely clustered.  This seems a powerful signal to me that, by spreading out geographically, we signal loyalty to a local firm or firm family.\n",
            "So what does such a strong signal to younger folks signal them?  It suggests, but is not a simple test, that they will not trust older firms and leaders more than people who trust others.  They are also presumably not as interested in knowing how firms manage risk, in who gives information, and what advice is needed.  I suspect their loyalty is more to their brand or \"identity\" without much to do with doing the right thing in the right situation.  \n",
            "<|endoftext|>\n",
            "\n",
            "Title: My Last Post\n",
            "Date: September 11, 2009 6:00 am\n",
            "Tags: Management\n",
            "Number of comments: 9\n",
            "\n",
            "My last post was a plea to businesses to find a stronger core, simple axiomatic statement to guide their practices to better integrate their different organizations.  Or they might be trying to think this statement while taking an axiomatic axiomatics test.\n",
            " I didn’t mean to sound bitter or hostile, but considering how I see some organizations today, I find that it is mostly the other way around.   When leaders seem most eager to appear loyal to a\n",
            "\n",
            "[1010 | 1826.42] loss=1.54 avg=1.58\n",
            "[1020 | 1908.42] loss=1.37 avg=1.57\n",
            "[1030 | 1990.48] loss=1.22 avg=1.55\n",
            "[1040 | 2073.05] loss=1.44 avg=1.55\n",
            "[1050 | 2155.10] loss=0.84 avg=1.52\n",
            "[1060 | 2237.58] loss=1.25 avg=1.50\n",
            "[1070 | 2319.85] loss=1.70 avg=1.51\n",
            "[1080 | 2402.00] loss=0.87 avg=1.49\n",
            "[1090 | 2484.32] loss=1.35 avg=1.48\n",
            "[1100 | 2566.93] loss=1.26 avg=1.47\n",
            "Saving checkpoint/run2/model-1100\n",
            "[1110 | 2655.10] loss=1.23 avg=1.46\n",
            "[1120 | 2736.97] loss=1.38 avg=1.46\n",
            "[1130 | 2819.11] loss=1.10 avg=1.45\n",
            "[1140 | 2901.37] loss=1.24 avg=1.44\n",
            "[1150 | 2983.64] loss=1.17 avg=1.43\n",
            "[1160 | 3066.21] loss=1.06 avg=1.42\n",
            "[1170 | 3148.76] loss=1.33 avg=1.42\n",
            "[1180 | 3231.19] loss=1.44 avg=1.42\n",
            "[1190 | 3313.53] loss=1.57 avg=1.42\n",
            "[1200 | 3395.83] loss=1.10 avg=1.41\n",
            "Saving checkpoint/run2/model-1200\n",
            "======== SAMPLE 1 ========\n",
            " in a paper in the August Journal of Cognitive Neuroscience. These are the kinds of findings that a better understanding of what is going on might enable clinicians to tailor drug therapy more precisely, and more liberally, to their patients.  Imagine how those researchers might have interpreted his result if it had involved such a subtle hint of a possible drug’s toxicity.  But it did not.  That isn’t typical.  \n",
            "<|endoftext|>\n",
            "\n",
            "Title: Who Pays\n",
            "Date: December 4, 2007 6:00 am\n",
            "Tags: Prediction Markets\n",
            "Number of comments: 6\n",
            "\n",
            "Today’s New York Times mentions \"a strange, obscure bet\" in which a betting agency hires an organization to \"help people select a side in a war between bad and very bad news.\"  I have long been puzzled by the fact that bets usually taken to win a race (or battle) on public policy often pay off on other measures, yet in bet world this is still considered creepy, and out of place.  Even asking \"Why the conflict?\" is bordering on the silly.  Imagine someone asked you why you take bets on public policy, or the merits of a particular political or religious candidate, but you would rightly point out their hypocrisy.  But most bet firms do not make their odds public, preferring to keep their prices proprietary.  As an aside, consider that in bet world a person would rather you didn’t take their odds, than their wage, than help pay for their gambling debts. I’d guess that for most employees of reputable businesses, such as sports officials, odds-makers would naturally gravitate toward the establishment side in a bet, relative to most employees who would prefer customers take a risk for their wage, or employee wage.  This does not seem remotely remotely “creepy” like the usual ethical compromises.  Yes you might prefer your employees don’t know your deeply held moral principles, but more typical you might prefer to make employees the wrong side of a bet (in other people’s wages, jobs, or personal career opportunities).\n",
            "<|endoftext|>\n",
            "\n",
            "Title: Consensus Too Real\n",
            "Date: December 4, 2007 6:00 am\n",
            "Tags: Psychology\n",
            "Number of comments: 18\n",
            "\n",
            "The latest Paper magazine has a cover story on \"The Fragile Disciple.\" It quotes Philip Tetlock:\n",
            "If the mind is fragile, then the body is almost certainly also fragile. . . . We lose our ability to walk, talk, laugh, and, indeed, most of the most basic bodily functions. … Even the appearance of fragility sometimes serves to reaffirm our commitment to idealism. Alcoholism is a classic example. … We eat less because we suspect that the body will be destroyed by the experience. … People with eating disorders are more likely than those who aren’t, to believe body death is very likely. … They are also, interestingly, more faithful to their marriages.  \"The Fragile Disciple\" author Alan J. Krueger.\n",
            "Tetlock seems to agree:  \"I do think alcoholics sometimes cheat on their spouses, but I think more on the level of the self than of the other person.\"\n",
            "Alcoholics’ problems seem more to be due to a combination of inability and wanting to follow the patterns of your previous behaviour, and an insensitivity to feeling bad via specific examples.  Tetlock agrees: \"I do think alcoholics’ sometimes cheat on their spouses, but more on the level of the self than of the other person.\"\n",
            "Both Tetlock and Jarding say they don’t agree with \"The Fragile Disciple\" author’s assessment of our level of fragility. Tiptock admits he won’t find a simple scientific test to definitively answer such a important question, and I’d guess most other academics similarly decides such a test will be hard.  Alas, these academics are unlikely to get involved as they mostly just don’t want to be part of the discussion.  All agreed we are fragile but also different; a certain degree of fragility is a necessary complement to other styles of thought.  Yet both Tetlock and Jarding say this is disgusting, and people who disagree with them are disgusting rude, dimwits, or mentally ill.  What could possibly be wrong with these judgments?  Any reason?  What’s wrong with feeling a little bit vulnerable to all this vulnerability and not quite being completely sure?  A little bit of certainty is good for a little buzz, right?  If such niceties don’t exist, maybe we should have them.  Why do contrarians complain so much more about people who disagree with them?\n",
            "I was\n",
            "\n",
            "[1210 | 3526.99] loss=0.99 avg=1.40\n",
            "[1220 | 3610.97] loss=1.12 avg=1.39\n",
            "[1230 | 3694.52] loss=0.81 avg=1.38\n",
            "[1240 | 3777.97] loss=1.03 avg=1.37\n",
            "[1250 | 3860.39] loss=1.05 avg=1.36\n",
            "[1260 | 3942.80] loss=0.80 avg=1.34\n",
            "[1270 | 4024.90] loss=1.13 avg=1.34\n",
            "[1280 | 4107.26] loss=1.12 avg=1.33\n",
            "[1290 | 4189.47] loss=1.14 avg=1.33\n",
            "[1300 | 4272.09] loss=1.48 avg=1.33\n",
            "Saving checkpoint/run2/model-1300\n",
            "[1310 | 4363.52] loss=0.94 avg=1.32\n",
            "[1320 | 4445.29] loss=0.98 avg=1.31\n",
            "[1330 | 4527.54] loss=0.65 avg=1.30\n",
            "[1340 | 4609.68] loss=1.27 avg=1.29\n",
            "[1350 | 4691.77] loss=0.92 avg=1.29\n",
            "[1360 | 4774.28] loss=0.95 avg=1.28\n",
            "[1370 | 4856.53] loss=1.20 avg=1.28\n",
            "[1380 | 4938.61] loss=0.99 avg=1.27\n",
            "[1390 | 5020.89] loss=1.16 avg=1.27\n",
            "[1400 | 5103.16] loss=0.97 avg=1.26\n",
            "Saving checkpoint/run2/model-1400\n",
            "======== SAMPLE 1 ========\n",
            " just for the fact that they were the first to discover a method.  There are lots of rules and norms we must comply with when writing software, but those can only get us so far, so let’s find a way to break some of them to see how we can break some of these norms.\n",
            "This isn’t easy.  Our mind is full of rules, and it is very hard to tell if they are actually rules we have to follow.  Many of these rules would be useful if we actually had to follow them, but many more would just waste space and resources if we gave them great authority.  Moreover, we are eager to collect a track record of agreeing to such rules, in order to impress potential associates or others. So we are unlikely to accept high authority rules just because they are “nice” by standard social standards.\n",
            "I see three ways to approach this problem:\n",
            "\n",
            "Rules can be either rules of evidence or rules of inference.\n",
            "Rules can either be norms binding ourselves or norms making others.\n",
            "Rulings can only override non-norms, so all other rules must apply. \n",
            "\n",
            "A huge benefit of having norms, over a big disadvantage of having norms, is that we can use norms to bind ourselves and infer others’ norms from our actions.  And we can infer other norm-bound behaviors as norms from our actions.   And this advantage is huge, big enough to easily outweigh the disadvantage of having weak standards.\n",
            "We can build in rules to reduce the info and size of our norms; we just ignore them when it makes sense to do so.  And we should try to do so, since norms are our main guidance about norm-behavior relationships.\n",
            "But it can be hard to actually bind ourselves to norms without much more effort.   Let me explain my personal policy of mostly ignoring norm-behavior precedence and ordering.  It started for me while I was a smart high school physics student, and continued as I worked my way through college.  I decided I wanted to do better than pretty much anyone else at an Interdisciplinary Science Fair I attended.\n",
            "I looked up various promising research topics, and tried to get help choosing among several smart candidates.  Sometimes I would beat one smart person to a table, and sometimes I would get folded up on my couch at the back of the room.  I kept stumbling over my answer, and then came up with several theories as to why it wouldn’t work.  These theories tended to rely heavily on common assumptions I didn’t have access to, even if the access was real.  Then I tried something new, by trying something new.\n",
            "This led to a series of less-funny theories where I’m not the only person who can see the problem.  I ended up asking for help from an “architect ” who had helped me try to design a system to achieve my original vision.  This architect put me up at an apartment until my research started to look bad, at which point I moved to a new city.\n",
            "<|endoftext|>\n",
            "\n",
            "Title: Why Open Thread 10\n",
            "Date: April 1, 2010 10:00 am\n",
            "Tags: Open Thread\n",
            "Number of comments: 20\n",
            "\n",
            "Here is our monthly place to discuss Overcoming Bias topics that have not appeared in the last 6 months.\n",
            "<|endoftext|>\n",
            "\n",
            "Title: What Evidence B\n",
            "Date: March 31, 2010 1:20 pm\n",
            "Tags: Meta\n",
            "Number of comments: 38\n",
            "\n",
            "Over at Criticality, Ross Douthat reviews the theory and practice of emotion regulation.  His main point is that we use lots of language to talk about what we want, and avoid the hard problems of showing and testing that we mean what we say.  When Douthat says something that sounds strange to most people, he will describe how different are non-Western cultures that speak this way, and explain how it might be strange to Westerners.  For example, in his new book Don’t Panic!, he says we do not show people the risks of financial market speculation:\n",
            "In our culture … to show people the risks of speculative markets, it’s not enough to just tell them that’s what you think.  It’s also enough to just to be provocative and to say that there is a risk.  It’s enough to just to suggest a policy that might be of use.  You have to do all of these things intentionally, deliberately, without actually being all that crazy. \n",
            "But we do not do these things merely as a way to get airtime.  We do them because, clearly, getting lots of attention doing silly things does not automatically make one to be a funny guy to talk to.  And the whole point of emotion regulation is to avoid seeming too crazy.\n",
            "This sort of view is apparently common among social phrenologists, and Douthat is\n",
            "\n",
            "[1410 | 5236.22] loss=0.80 avg=1.25\n",
            "[1420 | 5317.90] loss=0.84 avg=1.24\n",
            "[1430 | 5399.88] loss=0.73 avg=1.23\n",
            "[1440 | 5481.74] loss=1.23 avg=1.23\n",
            "[1450 | 5564.05] loss=0.57 avg=1.22\n",
            "[1460 | 5646.44] loss=1.13 avg=1.22\n",
            "[1470 | 5728.91] loss=0.80 avg=1.21\n",
            "[1480 | 5811.32] loss=1.23 avg=1.21\n",
            "[1490 | 5893.73] loss=1.04 avg=1.20\n",
            "interrupted\n",
            "Saving checkpoint/run2/model-1497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "12a9acc5-05ec-49c6-82f6-ecdbf9f051ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run2')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b4079cd6d56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mload_gpt2\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, multi_gpu)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n\u001b[0;32m--> 183\u001b[0;31m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n\u001b[0m\u001b[1;32m    184\u001b[0m         wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n\u001b[1;32m    185\u001b[0m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "8bbcb09e-5fe8-42af-9651-e4dc15118088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May  3 04:08:03 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert overcoming.json to overcoming.txt\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "with open(\"overcomingbias_cleaned_data.json\",'r') as f:\n",
        "  jsonfile = json.load(f)\n",
        "#gpt2.copy_file_from_gdrive(\"overcomingbias_cleaned_data.json\")\n",
        "\n",
        "with open(\"overcomingbias.txt\", \"w\") as new_file:\n",
        "\n",
        "  for k,v in jsonfile.items():\n",
        "    title = k[38:-5]\n",
        "    title = title.replace(\"-\", \" \").title()\n",
        "    print(\"Title: \" + title, file=new_file)\n",
        "    print(\"Date: \" + v[\"date\"], file=new_file)\n",
        "    print(\"Tags: \" + v[\"tags\"], file=new_file)\n",
        "    print(\"Number of comments: \" + v[\"num_comments\"], file=new_file)\n",
        "    print(\"\", file=new_file)\n",
        "    print(v[\"post_text\"], file=new_file)\n",
        "    print(\"<|endoftext|>\", file=new_file)\n",
        "    print(\"\", file=new_file)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}